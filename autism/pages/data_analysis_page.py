"""æ•°æ®åˆ†æé¡µé¢ - ä¿®å¤ç‰ˆæœ¬"""
import streamlit as st
import numpy as np

from autism.analysis import generate_clinical_analysis, get_behavior_summary_stats
from autism.ui_components.visualization import (
    create_correlation_scatter,
    display_abc_analysis,
    display_dsm5_analysis,
    display_comprehensive_comparison,
    display_statistical_analysis
)


def page_data_analysis():
    """æ•°æ®åˆ†æé¡µé¢ - æ”¯æŒåŒé‡è¯„ä¼°æ•°æ®åˆ†æ"""
    st.header("ğŸ“ˆ ä¸´åºŠæ•°æ®åˆ†æ")
    
    records = st.session_state.experiment_records
    
    if not records:
        st.warning("ğŸ“Š æš‚æ— è¯„ä¼°æ•°æ®ï¼Œè¯·å…ˆè¿›è¡Œä¸´åºŠè¯„ä¼°")
        st.stop()
    
    # æ·»åŠ æ•°æ®æ ¼å¼æ£€æŸ¥å’Œè¿ç§»åŠŸèƒ½
    if st.sidebar.button("ğŸ”§ ä¿®å¤æ•°æ®æ ¼å¼"):
        with st.spinner("æ­£åœ¨æ ‡å‡†åŒ–æ•°æ®æ ¼å¼..."):
            migrated_count = _migrate_data_format()
            if migrated_count > 0:
                st.success(f"âœ… æˆåŠŸæ ‡å‡†åŒ– {migrated_count} æ¡è®°å½•")
                st.rerun()
            else:
                st.info("ğŸ“Š æ‰€æœ‰æ•°æ®å·²ç»æ˜¯æ ‡å‡†æ ¼å¼")
    
    # ç”Ÿæˆåˆ†æ
    analysis = generate_clinical_analysis(records)
    
    # æ•°æ®æ¦‚è§ˆ
    st.subheader("ğŸ¥ è¯„ä¼°æ•°æ®æ¦‚è§ˆ")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("è¯„ä¼°æ€»æ•°", len(records))
    with col2:
        st.metric("è¯„ä¼°æƒ…å¢ƒæ•°", len(set(r['scene'] for r in records)))
    with col3:
        # è®¡ç®—å¹³å‡ABCæ€»åˆ† - æ·»åŠ æ ¼å¼å…¼å®¹æ€§æ£€æŸ¥
        abc_scores = []
        for r in records:
            if 'abc_evaluation' in r:
                # æ–°æ ¼å¼
                abc_scores.append(r['abc_evaluation']['total_score'])
            elif 'abc_total_score' in r:
                # æ—§æ ¼å¼
                abc_scores.append(r['abc_total_score'])
            elif 'evaluation_scores' in r and r.get('assessment_standard') == 'ABC':
                # è®¡ç®—æ—§æ ¼å¼çš„æ€»åˆ†
                total = sum(r['evaluation_scores'].values())
                abc_scores.append(total)
        
        if abc_scores:
            avg_abc = np.mean(abc_scores)
            st.metric("å¹³å‡ABCæ€»åˆ†", f"{avg_abc:.1f}")
        else:
            st.metric("å¹³å‡ABCæ€»åˆ†", "N/A")
            
    with col4:
        # è®¡ç®—å¹³å‡DSM-5æ ¸å¿ƒç—‡çŠ¶ - æ·»åŠ æ ¼å¼å…¼å®¹æ€§æ£€æŸ¥
        dsm5_scores = []
        for r in records:
            if 'dsm5_evaluation' in r:
                # æ–°æ ¼å¼
                dsm5_scores.append(r['dsm5_evaluation']['core_symptom_average'])
            elif 'dsm5_core_symptom_average' in r:
                # æ—§æ ¼å¼
                dsm5_scores.append(r['dsm5_core_symptom_average'])
            elif 'evaluation_scores' in r and r.get('assessment_standard') == 'DSM5':
                # è®¡ç®—æ—§æ ¼å¼çš„æ ¸å¿ƒç—‡çŠ¶å‡åˆ†
                scores = r['evaluation_scores']
                core_metrics = ['ç¤¾äº¤æƒ…æ„Ÿäº’æƒ ç¼ºé™·', 'éè¨€è¯­äº¤æµç¼ºé™·', 
                              'å‘å±•ç»´æŒå…³ç³»ç¼ºé™·', 'åˆ»æ¿é‡å¤åŠ¨ä½œ']
                core_scores = [scores.get(m, 0) for m in core_metrics if m in scores]
                if core_scores:
                    dsm5_scores.append(np.mean(core_scores))
        
        if dsm5_scores:
            avg_dsm5 = np.mean(dsm5_scores)
            st.metric("å¹³å‡DSM-5æ ¸å¿ƒ", f"{avg_dsm5:.2f}")
        else:
            st.metric("å¹³å‡DSM-5æ ¸å¿ƒ", "N/A")
    
    # è¯„ä¼°ä¸€è‡´æ€§åˆ†æ - åªåˆ†æåŒ…å«ä¸¤ç§è¯„ä¼°çš„è®°å½•
    unified_records = [r for r in records if 
                      ('abc_evaluation' in r and 'dsm5_evaluation' in r) or
                      ('abc_total_score' in r and 'dsm5_core_symptom_average' in r)]
    
    if unified_records:
        st.subheader("ğŸ”„ ABCä¸DSM-5è¯„ä¼°ä¸€è‡´æ€§åˆ†æ")
        
        consistency_results = _analyze_consistency(unified_records)
        
        col_cons1, col_cons2, col_cons3 = st.columns(3)
        with col_cons1:
            st.metric("Pearsonç›¸å…³ç³»æ•°", f"{consistency_results['correlation']:.3f}")
            st.write(f"på€¼: {consistency_results['p_value']:.4f}")
        with col_cons2:
            st.metric("ä¸€è‡´æ€§æ¯”ä¾‹", f"{consistency_results['agreement_rate']:.1f}%")
            st.write("(ä¸¥é‡ç¨‹åº¦åˆ¤æ–­ä¸€è‡´)")
        with col_cons3:
            st.metric("è¯„åˆ†å·®å¼‚", f"{consistency_results['mean_difference']:.2f}")
            st.write("(æ ‡å‡†åŒ–å)")
        
        # æ•£ç‚¹å›¾æ˜¾ç¤ºç›¸å…³æ€§
        fig_scatter = create_correlation_scatter(unified_records)
        st.plotly_chart(fig_scatter, use_container_width=True)
    else:
        st.info("ğŸ“Š æ²¡æœ‰åŒ…å«åŒé‡è¯„ä¼°çš„è®°å½•ï¼Œæ— æ³•è¿›è¡Œä¸€è‡´æ€§åˆ†æ")
    
    # åˆ†åˆ«æ˜¾ç¤ºä¸¤ç§è¯„ä¼°çš„ç»“æœ
    tab1, tab2, tab3 = st.tabs(["ABCé‡è¡¨åˆ†æ", "DSM-5æ ‡å‡†åˆ†æ", "ç»¼åˆå¯¹æ¯”"])
    
    with tab1:
        display_abc_analysis(records, analysis)
    
    with tab2:
        display_dsm5_analysis(records, analysis)
    
    with tab3:
        display_comprehensive_comparison(records, analysis)
    
    # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    if len(records) > 10:
        st.subheader("ğŸ“ ç»Ÿè®¡å­¦åˆ†æ")
        display_statistical_analysis(records)


def _analyze_consistency(records):
    """åˆ†æABCå’ŒDSM-5è¯„ä¼°çš„ä¸€è‡´æ€§ - æ”¯æŒæ–°æ—§æ ¼å¼"""
    from scipy import stats
    
    abc_scores = []
    dsm5_scores = []
    agreements = 0
    
    for record in records:
        # è·å–ABCæ€»åˆ† - æ”¯æŒæ–°æ—§æ ¼å¼
        if 'abc_evaluation' in record:
            abc_total = record['abc_evaluation']['total_score']
        elif 'abc_total_score' in record:
            abc_total = record['abc_total_score']
        else:
            # ä»evaluation_scoresè®¡ç®—
            abc_total = sum(record.get('evaluation_scores', {}).values())
        
        # è·å–DSM5æ ¸å¿ƒç—‡çŠ¶å‡åˆ† - æ”¯æŒæ–°æ—§æ ¼å¼
        if 'dsm5_evaluation' in record:
            dsm5_core = record['dsm5_evaluation']['core_symptom_average']
        elif 'dsm5_core_symptom_average' in record:
            dsm5_core = record['dsm5_core_symptom_average']
        else:
            # ä»evaluation_scoresè®¡ç®—
            scores = record.get('evaluation_scores', {})
            core_metrics = ['ç¤¾äº¤æƒ…æ„Ÿäº’æƒ ç¼ºé™·', 'éè¨€è¯­äº¤æµç¼ºé™·', 
                          'å‘å±•ç»´æŒå…³ç³»ç¼ºé™·', 'åˆ»æ¿é‡å¤åŠ¨ä½œ']
            core_scores = [scores.get(m, 0) for m in core_metrics if m in scores]
            dsm5_core = np.mean(core_scores) if core_scores else 0
        
        # æ ‡å‡†åŒ–åˆ†æ•°
        abc_normalized = abc_total / 158
        dsm5_normalized = dsm5_core / 5
        
        abc_scores.append(abc_normalized)
        dsm5_scores.append(dsm5_normalized)
        
        # åˆ¤æ–­ä¸€è‡´æ€§
        abc_severe = abc_total >= 67
        dsm5_severe = dsm5_core >= 3.5
        if abc_severe == dsm5_severe:
            agreements += 1
    
    # è®¡ç®—ç›¸å…³æ€§å’Œpå€¼
    if len(records) > 1:
        correlation, p_value = stats.pearsonr(abc_scores, dsm5_scores)
    else:
        correlation, p_value = 0, 1
    
    return {
        'correlation': correlation,
        'p_value': p_value,
        'agreement_rate': (agreements / len(records)) * 100 if records else 0,
        'mean_difference': np.mean([abs(a - d) for a, d in zip(abc_scores, dsm5_scores)])
    }


def _migrate_data_format():
    """è¿ç§»æ‰€æœ‰è®°å½•åˆ°æ ‡å‡†æ ¼å¼"""
    records = st.session_state.experiment_records
    migrated_count = 0
    
    # é¢†åŸŸåç§°æ˜ å°„
    domain_mapping = {
        'æ„Ÿè§‰': 'æ„Ÿè§‰é¢†åŸŸå¾—åˆ†',
        'äº¤å¾€': 'äº¤å¾€é¢†åŸŸå¾—åˆ†',
        'èº¯ä½“è¿åŠ¨': 'èº¯ä½“è¿åŠ¨é¢†åŸŸå¾—åˆ†',
        'è¿åŠ¨': 'èº¯ä½“è¿åŠ¨é¢†åŸŸå¾—åˆ†',
        'è¯­è¨€': 'è¯­è¨€é¢†åŸŸå¾—åˆ†',
        'ç¤¾äº¤ä¸è‡ªç†': 'ç¤¾äº¤ä¸è‡ªç†é¢†åŸŸå¾—åˆ†',
        'è‡ªç†': 'ç¤¾äº¤ä¸è‡ªç†é¢†åŸŸå¾—åˆ†'
    }
    
    for record in records:
        needs_migration = False
        
        # æ£€æŸ¥å¹¶è¿ç§»ABCè¯„ä¼°æ•°æ®
        if 'abc_evaluation' in record and 'domain_scores' in record['abc_evaluation']:
            domain_scores = record['abc_evaluation']['domain_scores']
            new_domain_scores = {}
            
            for domain, score in domain_scores.items():
                # æ ‡å‡†åŒ–é¢†åŸŸåç§°
                if domain in domain_mapping:
                    normalized_domain = domain_mapping[domain]
                    needs_migration = True
                elif not domain.endswith('é¢†åŸŸå¾—åˆ†'):
                    normalized_domain = f"{domain}é¢†åŸŸå¾—åˆ†"
                    needs_migration = True
                else:
                    normalized_domain = domain
                
                new_domain_scores[normalized_domain] = score
            
            if needs_migration:
                record['abc_evaluation']['domain_scores'] = new_domain_scores
                migrated_count += 1
        
        # å¤„ç†æ—§æ ¼å¼æ•°æ®
        elif 'evaluation_scores' in record and record.get('assessment_standard') == 'ABC':
            # è½¬æ¢ä¸ºæ–°æ ¼å¼
            scores = record['evaluation_scores']
            total_score = sum(scores.values())
            
            # è®¡ç®—é¢†åŸŸåˆ†æ•°
            domain_scores = {}
            
            # å°è¯•ä»evaluation_scoresä¸­æå–
            for key, value in scores.items():
                if key in domain_mapping:
                    normalized_key = domain_mapping[key]
                    domain_scores[normalized_key] = value
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤è®¡ç®—
            if not domain_scores:
                domain_scores = {
                    'æ„Ÿè§‰é¢†åŸŸå¾—åˆ†': sum(scores.get(item, 0) for item in ['æ„Ÿè§‰', 'æ„Ÿè§‰å¼‚å¸¸'] if item in scores),
                    'äº¤å¾€é¢†åŸŸå¾—åˆ†': sum(scores.get(item, 0) for item in ['äº¤å¾€', 'äººé™…äº¤å¾€å›°éš¾'] if item in scores),
                    'èº¯ä½“è¿åŠ¨é¢†åŸŸå¾—åˆ†': scores.get('èº¯ä½“è¿åŠ¨', 0),
                    'è¯­è¨€é¢†åŸŸå¾—åˆ†': sum(scores.get(item, 0) for item in ['è¯­è¨€', 'é‡å¤æ€§è¯­è¨€'] if item in scores),
                    'ç¤¾äº¤ä¸è‡ªç†é¢†åŸŸå¾—åˆ†': scores.get('ç”Ÿæ´»è‡ªç†', 0)
                }
            
            # è®¡ç®—ä¸¥é‡ç¨‹åº¦
            if total_score <= 30:
                severity = 'æ— æ˜æ˜¾ç—‡çŠ¶'
            elif total_score <= 67:
                severity = 'è½»åº¦'
            elif total_score <= 100:
                severity = 'ä¸­åº¦'
            else:
                severity = 'é‡åº¦'
            
            # åˆ›å»ºæ–°æ ¼å¼çš„è¯„ä¼°æ•°æ®
            record['abc_evaluation'] = {
                'total_score': total_score,
                'severity': severity,
                'domain_scores': domain_scores,
                'identified_behaviors': {}
            }
            
            # å¦‚æœæ²¡æœ‰DSM5è¯„ä¼°ï¼Œæ·»åŠ ç©ºçš„
            if 'dsm5_evaluation' not in record:
                record['dsm5_evaluation'] = {
                    'core_symptom_average': 0,
                    'scores': {},
                    'note': 'æ­¤è®°å½•ä»…åŒ…å«ABCè¯„ä¼°'
                }
            
            migrated_count += 1
    
    return migrated_count