"""
å¢å¼ºç‰ˆæ‰¹é‡ç ”ç©¶é¡µé¢ - æ”¯æŒå¤šé‡è¡¨é€‰æ‹©çš„æ‰¹é‡è¯„ä¼°
"""
import streamlit as st
import pandas as pd
import numpy as np
import time
import datetime

from common.batch_processor import run_batch_processing
from autism.configs import UNIFIED_AUTISM_PROFILES, CLINICAL_SCENE_CONFIG
from autism.evaluation.enhanced_unified_evaluator import (
    run_enhanced_experiment,
    AVAILABLE_SCALES,
    DEFAULT_SCALES,
    COMPREHENSIVE_SCALES
)
from autism.ui_components.visualization import create_assessment_comparison_plot
from autism.ui_components.result_display import analyze_batch_consistency


def generate_enhanced_experiment_batch(
    severity_profiles: dict,
    scene_configs: dict,
    repeats: int,
    selected_scales: list
) -> list:
    """
    ç”Ÿæˆå¢å¼ºç‰ˆå®éªŒæ‰¹æ¬¡é…ç½®
    
    Args:
        severity_profiles: ä¸¥é‡ç¨‹åº¦é…ç½®å­—å…¸
        scene_configs: åœºæ™¯é…ç½®å­—å…¸
        repeats: æ¯ä¸ªç»„åˆçš„é‡å¤æ¬¡æ•°
        selected_scales: é€‰æ‹©çš„è¯„ä¼°é‡è¡¨
    
    Returns:
        å®éªŒé…ç½®åˆ—è¡¨
    """
    experiments = []
    counter = 0
    
    for severity_name, profile in severity_profiles.items():
        for scene_name, scene_data in scene_configs.items():
            for repeat in range(repeats):
                for activity in scene_data['activities'][:2]:  # æ¯ä¸ªåœºæ™¯é€‰2ä¸ªæ´»åŠ¨
                    for trigger in scene_data['triggers'][:2]:  # æ¯ä¸ªæ´»åŠ¨é€‰2ä¸ªè§¦å‘å› ç´ 
                        counter += 1
                        
                        experiment_config = {
                            'experiment_id': f"BATCH_{counter:04d}_{severity_name[:4]}_{scene_name[:4]}",
                            'template': severity_name,
                            'scene': scene_name,
                            'activity': activity,
                            'trigger': trigger,
                            'autism_profile': profile.copy(),
                            'selected_scales': selected_scales,
                            'batch_number': counter,
                            'repeat_number': repeat + 1
                        }
                        experiments.append(experiment_config)
    
    return experiments


def page_batch_research_enhanced():
    """å¢å¼ºç‰ˆæ‰¹é‡ç ”ç©¶é¡µé¢ - æ”¯æŒå¤šé‡è¡¨é€‰æ‹©"""
    st.header("ğŸ”¬ æ‰¹é‡ä¸´åºŠç ”ç©¶ï¼ˆå¤šé‡è¡¨ç‰ˆï¼‰")
    st.markdown("æ‰¹é‡ç”Ÿæˆè¡Œä¸ºæ ·æœ¬ï¼Œä½¿ç”¨å¤šä¸ªé‡è¡¨è¿›è¡Œç»¼åˆè¯„ä¼°å’Œå¯¹æ¯”åˆ†æ")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ¯ ç ”ç©¶è®¾è®¡")
        
        # é‡è¡¨é€‰æ‹©
        st.write("### 1ï¸âƒ£ é€‰æ‹©è¯„ä¼°é‡è¡¨")
        
        scale_preset = st.radio(
            "é‡è¡¨ç»„åˆæ–¹æ¡ˆ",
            ["åŒé‡è¡¨å¯¹æ¯”ï¼ˆABC+DSM5ï¼‰", "ä¸‰é‡è¡¨éªŒè¯ï¼ˆABC+DSM5+CARSï¼‰", 
             "å…¨é‡è¡¨ç»¼åˆï¼ˆæ‰€æœ‰4ä¸ªï¼‰", "è‡ªå®šä¹‰é€‰æ‹©"],
            help="é€‰æ‹©æ‰¹é‡ç ”ç©¶ä½¿ç”¨çš„é‡è¡¨ç»„åˆ"
        )
        
        if scale_preset == "åŒé‡è¡¨å¯¹æ¯”ï¼ˆABC+DSM5ï¼‰":
            selected_scales = ['ABC', 'DSM5']
            st.info("âœ… ç»å…¸åŒé‡è¡¨å¯¹æ¯”ç ”ç©¶")
        elif scale_preset == "ä¸‰é‡è¡¨éªŒè¯ï¼ˆABC+DSM5+CARSï¼‰":
            selected_scales = ['ABC', 'DSM5', 'CARS']
            st.info("âœ… ä¸‰é‡è¡¨äº¤å‰éªŒè¯ç ”ç©¶")
        elif scale_preset == "å…¨é‡è¡¨ç»¼åˆï¼ˆæ‰€æœ‰4ä¸ªï¼‰":
            selected_scales = COMPREHENSIVE_SCALES
            st.warning("âš ï¸ å…¨é‡è¡¨è¯„ä¼°ï¼Œå¤„ç†æ—¶é—´è¾ƒé•¿")
        else:
            selected_scales = []
            cols = st.columns(4)
            for idx, (scale_key, scale_info) in enumerate(AVAILABLE_SCALES.items()):
                with cols[idx]:
                    if st.checkbox(scale_info['name'], value=(scale_key in DEFAULT_SCALES)):
                        selected_scales.append(scale_key)
        
        # ç ”ç©¶è§„æ¨¡
        st.write("### 2ï¸âƒ£ é€‰æ‹©ç ”ç©¶è§„æ¨¡")
        
        research_scale = st.radio(
            "ç ”ç©¶è§„æ¨¡",
            ["è¯•ç‚¹ç ”ç©¶ï¼ˆæ¨èï¼‰", "æ ‡å‡†ç ”ç©¶", "å¤§æ ·æœ¬ç ”ç©¶", "è‡ªå®šä¹‰"],
            help="æ ¹æ®ç ”ç©¶éœ€è¦é€‰æ‹©åˆé€‚çš„æ ·æœ¬è§„æ¨¡"
        )
        
        if research_scale == "è¯•ç‚¹ç ”ç©¶ï¼ˆæ¨èï¼‰":
            default_severities = list(UNIFIED_AUTISM_PROFILES.keys())[:2]
            default_contexts = list(CLINICAL_SCENE_CONFIG.keys())[:2]
            default_repeats = 1
            estimated_samples = len(default_severities) * len(default_contexts) * 4 * default_repeats
            st.info(f"ğŸš€ é¢„è®¡ç”Ÿæˆ {estimated_samples} ä¸ªæ ·æœ¬ï¼Œçº¦éœ€ {estimated_samples * len(selected_scales) * 0.5:.0f} åˆ†é’Ÿ")
        elif research_scale == "æ ‡å‡†ç ”ç©¶":
            default_severities = list(UNIFIED_AUTISM_PROFILES.keys())[:3]
            default_contexts = list(CLINICAL_SCENE_CONFIG.keys())[:3]
            default_repeats = 2
            estimated_samples = len(default_severities) * len(default_contexts) * 4 * default_repeats
            st.info(f"â³ é¢„è®¡ç”Ÿæˆ {estimated_samples} ä¸ªæ ·æœ¬ï¼Œçº¦éœ€ {estimated_samples * len(selected_scales) * 0.5:.0f} åˆ†é’Ÿ")
        elif research_scale == "å¤§æ ·æœ¬ç ”ç©¶":
            default_severities = list(UNIFIED_AUTISM_PROFILES.keys())
            default_contexts = list(CLINICAL_SCENE_CONFIG.keys())
            default_repeats = 2
            estimated_samples = len(default_severities) * len(default_contexts) * 4 * default_repeats
            st.warning(f"â° é¢„è®¡ç”Ÿæˆ {estimated_samples} ä¸ªæ ·æœ¬ï¼Œçº¦éœ€ {estimated_samples * len(selected_scales) * 0.5:.0f} åˆ†é’Ÿ")
        else:
            # è‡ªå®šä¹‰é€‰æ‹©
            default_severities = []
            default_contexts = []
            default_repeats = 1
            
            st.write("**é€‰æ‹©ä¸¥é‡ç¨‹åº¦ç»„ï¼š**")
            severity_cols = st.columns(3)
            for idx, severity in enumerate(UNIFIED_AUTISM_PROFILES.keys()):
                with severity_cols[idx % 3]:
                    if st.checkbox(severity, key=f"sev_{severity}"):
                        default_severities.append(severity)
            
            st.write("**é€‰æ‹©è¯„ä¼°æƒ…å¢ƒï¼š**")
            context_cols = st.columns(3)
            for idx, context in enumerate(CLINICAL_SCENE_CONFIG.keys()):
                with context_cols[idx % 3]:
                    if st.checkbox(context, key=f"ctx_{context}"):
                        default_contexts.append(context)
            
            default_repeats = st.slider("æ¯ç»„åˆé‡å¤æ¬¡æ•°", 1, 5, 1)
            
            if default_severities and default_contexts:
                estimated_samples = len(default_severities) * len(default_contexts) * 4 * default_repeats
                st.info(f"é¢„è®¡ç”Ÿæˆ {estimated_samples} ä¸ªæ ·æœ¬")
        
        # æœ€ç»ˆé€‰æ‹©ç¡®è®¤
        if research_scale != "è‡ªå®šä¹‰":
            selected_severities = st.multiselect(
                "ç¡®è®¤ä¸¥é‡ç¨‹åº¦ç»„", 
                list(UNIFIED_AUTISM_PROFILES.keys()),
                default=default_severities
            )
            
            selected_contexts = st.multiselect(
                "ç¡®è®¤è¯„ä¼°æƒ…å¢ƒ",
                list(CLINICAL_SCENE_CONFIG.keys()),
                default=default_contexts
            )
            
            repeats_per_combo = st.slider(
                "æ¯ç»„åˆé‡å¤æ¬¡æ•°", 
                1, 5, 
                default_repeats,
                help="å¢åŠ é‡å¤æ¬¡æ•°æé«˜ç»Ÿè®¡å¯é æ€§"
            )
        else:
            selected_severities = default_severities
            selected_contexts = default_contexts
            repeats_per_combo = default_repeats
        
        # ç ”ç©¶é€‰é¡¹
        st.write("### 3ï¸âƒ£ ç ”ç©¶é€‰é¡¹")
        
        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            compare_scales = st.checkbox("ç”Ÿæˆé‡è¡¨é—´å¯¹æ¯”åˆ†æ", value=True)
            calculate_reliability = st.checkbox("è®¡ç®—è¯„ä¼°ä¿¡åº¦", value=True)
        with col_opt2:
            export_raw_data = st.checkbox("å¯¼å‡ºåŸå§‹æ•°æ®", value=True)
            generate_report = st.checkbox("ç”Ÿæˆç ”ç©¶æŠ¥å‘Š", value=True)
    
    with col2:
        st.subheader("ğŸ“Š ç ”ç©¶ä¿¡æ¯")
        
        if selected_severities and selected_contexts and selected_scales:
            # æ˜¾ç¤ºç ”ç©¶è®¾è®¡çŸ©é˜µ
            st.write("**ç ”ç©¶è®¾è®¡çŸ©é˜µï¼š**")
            
            design_matrix = pd.DataFrame(
                index=selected_severities,
                columns=selected_contexts,
                data=[[repeats_per_combo] * len(selected_contexts)] * len(selected_severities)
            )
            st.dataframe(design_matrix, use_container_width=True)
            
            # è®¡ç®—æ€»æ ·æœ¬æ•°
            total_samples = (
                len(selected_severities) * 
                len(selected_contexts) * 
                4 * repeats_per_combo  # 4æ˜¯æ´»åŠ¨å’Œè§¦å‘å› ç´ çš„ç»„åˆ
            )
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            st.metric("æ€»æ ·æœ¬æ•°", total_samples)
            st.metric("è¯„ä¼°é‡è¡¨æ•°", len(selected_scales))
            st.metric("æ€»è¯„ä¼°æ¬¡æ•°", total_samples * len(selected_scales))
            
            # é¢„è®¡æ—¶é—´
            estimated_time = total_samples * len(selected_scales) * 0.5  # å‡è®¾æ¯ä¸ªè¯„ä¼°0.5åˆ†é’Ÿ
            st.metric("é¢„è®¡æ—¶é—´", f"{estimated_time:.0f} åˆ†é’Ÿ")
            
            # æ˜¾ç¤ºé€‰ä¸­çš„é‡è¡¨
            st.write("**ä½¿ç”¨çš„é‡è¡¨ï¼š**")
            for scale in selected_scales:
                st.write(f"â€¢ {AVAILABLE_SCALES[scale]['name']}")
        else:
            st.warning("è¯·å®Œæˆç ”ç©¶è®¾è®¡")
        
        # å¼€å§‹æ‰¹é‡ç ”ç©¶æŒ‰é’®
        st.write("---")
        
        if st.button("ğŸš€ å¼€å§‹æ‰¹é‡ç ”ç©¶", type="primary", use_container_width=True):
            if not selected_severities:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªä¸¥é‡ç¨‹åº¦ç»„")
            elif not selected_contexts:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªè¯„ä¼°æƒ…å¢ƒ")
            elif not selected_scales:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªè¯„ä¼°é‡è¡¨")
            else:
                run_batch_research(
                    selected_severities,
                    selected_contexts,
                    repeats_per_combo,
                    selected_scales,
                    compare_scales,
                    calculate_reliability,
                    export_raw_data,
                    generate_report
                )


def run_batch_research(
    severities: list,
    contexts: list,
    repeats: int,
    scales: list,
    compare: bool,
    reliability: bool,
    export_data: bool,
    report: bool
):
    """æ‰§è¡Œæ‰¹é‡ç ”ç©¶"""
    
    # å‡†å¤‡é…ç½®
    severity_dict = {k: UNIFIED_AUTISM_PROFILES[k] for k in severities}
    context_dict = {k: CLINICAL_SCENE_CONFIG[k] for k in contexts}
    
    # ç”Ÿæˆå®éªŒæ‰¹æ¬¡
    experiments = generate_enhanced_experiment_batch(
        severity_dict,
        context_dict,
        repeats,
        scales
    )
    
    st.info(f"ğŸ“Š å°†ç”Ÿæˆ {len(experiments)} ä¸ªå®éªŒ")
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    results_container = st.container()
    
    # æ‰§è¡Œæ‰¹é‡å¤„ç†
    successful = 0
    failed = 0
    results = []
    
    for idx, experiment_config in enumerate(experiments):
        # æ›´æ–°è¿›åº¦
        progress = (idx + 1) / len(experiments)
        progress_bar.progress(progress)
        status_text.text(f"å¤„ç†ä¸­... ({idx + 1}/{len(experiments)})")
        
        try:
            # æ‰§è¡Œå®éªŒ
            result = run_enhanced_experiment(experiment_config)
            
            if 'error' not in result:
                results.append(result)
                successful += 1
                st.session_state.experiment_records.append(result)
            else:
                failed += 1
                
        except Exception as e:
            failed += 1
            print(f"å®éªŒå¤±è´¥: {e}")
        
        # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
        time.sleep(0.5)
    
    # å®Œæˆå¤„ç†
    progress_bar.progress(1.0)
    status_text.text("å¤„ç†å®Œæˆï¼")
    
    # æ˜¾ç¤ºç»“æœ
    with results_container:
        st.success(f"âœ… æ‰¹é‡ç ”ç©¶å®Œæˆï¼æˆåŠŸ: {successful}, å¤±è´¥: {failed}")
        
        if results:
            # åˆ†æç»“æœ
            st.subheader("ğŸ“Š ç ”ç©¶ç»“æœåˆ†æ")
            
            # é‡è¡¨é—´å¯¹æ¯”
            if compare and len(scales) > 1:
                display_scale_comparison_analysis(results, scales)
            
            # ä¿¡åº¦åˆ†æ
            if reliability:
                display_reliability_analysis(results, scales)
            
            # æ•°æ®å¯¼å‡º
            if export_data:
                export_research_data(results, scales)
            
            # ç”ŸæˆæŠ¥å‘Š
            if report:
                generate_research_report(results, scales, severities, contexts)


def display_scale_comparison_analysis(results: list, scales: list):
    """æ˜¾ç¤ºé‡è¡¨é—´å¯¹æ¯”åˆ†æ"""
    st.write("### é‡è¡¨é—´ä¸€è‡´æ€§åˆ†æ")
    
    # æ”¶é›†å¯¹æ¯”æ•°æ®
    comparison_data = []
    
    for result in results:
        if 'scale_comparison' in result:
            comp = result['scale_comparison']
            if 'consistency' in comp:
                comparison_data.append({
                    'ID': result['experiment_id'][:10],
                    'ä¸€è‡´æ€§': comp['consistency'].get('overall', 'N/A'),
                    'ä¸¥é‡ç¨‹åº¦': result['template'],
                    'åœºæ™¯': result['scene']
                })
    
    if comparison_data:
        comp_df = pd.DataFrame(comparison_data)
        
        # ç»Ÿè®¡ä¸€è‡´æ€§
        consistency_stats = comp_df['ä¸€è‡´æ€§'].value_counts()
        
        col1, col2 = st.columns(2)
        with col1:
            st.write("**ä¸€è‡´æ€§åˆ†å¸ƒï¼š**")
            st.bar_chart(consistency_stats)
        
        with col2:
            st.write("**ä¸€è‡´æ€§ç™¾åˆ†æ¯”ï¼š**")
            for level, count in consistency_stats.items():
                percentage = (count / len(comparison_data)) * 100
                st.write(f"â€¢ {level}: {percentage:.1f}%")


def display_reliability_analysis(results: list, scales: list):
    """æ˜¾ç¤ºä¿¡åº¦åˆ†æ"""
    st.write("### è¯„ä¼°ä¿¡åº¦åˆ†æ")
    
    # æŒ‰é‡è¡¨è®¡ç®—ä¿¡åº¦æŒ‡æ ‡
    for scale in scales:
        scale_scores = []
        
        for result in results:
            if scale == 'ABC' and 'abc_evaluation' in result:
                scale_scores.append(result['abc_evaluation']['total_score'])
            elif scale == 'DSM5' and 'dsm5_evaluation' in result:
                scale_scores.append(result['dsm5_evaluation']['core_symptom_average'])
            elif scale == 'CARS' and 'cars_evaluation' in result:
                scale_scores.append(result['cars_evaluation']['total_score'])
            elif scale == 'ASSQ' and 'assq_evaluation' in result:
                scale_scores.append(result['assq_evaluation']['total_score'])
        
        if scale_scores:
            # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
            mean_score = np.mean(scale_scores)
            std_score = np.std(scale_scores)
            cv = (std_score / mean_score) * 100 if mean_score > 0 else 0
            
            st.write(f"**{AVAILABLE_SCALES[scale]['name']}ï¼š**")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å‡å€¼", f"{mean_score:.2f}")
            with col2:
                st.metric("æ ‡å‡†å·®", f"{std_score:.2f}")
            with col3:
                st.metric("å˜å¼‚ç³»æ•°", f"{cv:.1f}%")


def export_research_data(results: list, scales: list):
    """å¯¼å‡ºç ”ç©¶æ•°æ®"""
    st.write("### æ•°æ®å¯¼å‡º")
    
    # å‡†å¤‡å¯¼å‡ºæ•°æ®
    export_data = []
    
    for result in results:
        row = {
            'ID': result['experiment_id'],
            'æ—¶é—´': result['timestamp'].strftime('%Y-%m-%d %H:%M:%S'),
            'ä¸¥é‡ç¨‹åº¦': result['template'],
            'åœºæ™¯': result['scene'],
            'æ´»åŠ¨': result['activity']
        }
        
        # æ·»åŠ å„é‡è¡¨å¾—åˆ†
        if 'ABC' in scales and 'abc_evaluation' in result:
            row['ABCæ€»åˆ†'] = result['abc_evaluation']['total_score']
            row['ABCä¸¥é‡åº¦'] = result['abc_evaluation']['severity']
        
        if 'DSM5' in scales and 'dsm5_evaluation' in result:
            row['DSM5æ ¸å¿ƒ'] = result['dsm5_evaluation']['core_symptom_average']
            row['DSM5ç¨‹åº¦'] = result['dsm5_evaluation']['severity_level']
        
        if 'CARS' in scales and 'cars_evaluation' in result:
            row['CARSæ€»åˆ†'] = result['cars_evaluation']['total_score']
            row['CARSä¸¥é‡åº¦'] = result['cars_evaluation']['severity']
        
        if 'ASSQ' in scales and 'assq_evaluation' in result:
            row['ASSQæ€»åˆ†'] = result['assq_evaluation']['total_score']
            row['ASSQé£é™©'] = result['assq_evaluation']['risk_level']
        
        export_data.append(row)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(export_data)
    
    # æä¾›ä¸‹è½½
    csv = df.to_csv(index=False, encoding='utf-8-sig')
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½CSVæ•°æ®",
        data=csv,
        file_name=f"batch_research_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime='text/csv'
    )


def generate_research_report(results: list, scales: list, severities: list, contexts: list):
    """ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"""
    st.write("### ç ”ç©¶æŠ¥å‘Š")
    
    report = []
    report.append("# æ‰¹é‡ä¸´åºŠç ”ç©¶æŠ¥å‘Š")
    report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    report.append("## ç ”ç©¶è®¾è®¡")
    report.append(f"- æ ·æœ¬æ•°: {len(results)}")
    report.append(f"- ä¸¥é‡ç¨‹åº¦ç»„: {', '.join(severities)}")
    report.append(f"- è¯„ä¼°æƒ…å¢ƒ: {', '.join(contexts)}")
    report.append(f"- ä½¿ç”¨é‡è¡¨: {', '.join([AVAILABLE_SCALES[s]['name'] for s in scales])}")
    report.append("")
    
    # æ·»åŠ æ›´å¤šæŠ¥å‘Šå†…å®¹...
    
    report_text = "\n".join(report)
    
    st.download_button(
        label="ğŸ“„ ä¸‹è½½ç ”ç©¶æŠ¥å‘Š",
        data=report_text,
        file_name=f"research_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
        mime='text/markdown'
    )