"""
å¤šé‡è¡¨è¯„ä¼°é¡µé¢ - æ”¯æŒè‡ªå®šä¹‰é‡è¡¨é€‰æ‹©
"""
import streamlit as st
import datetime
import pandas as pd

from autism.configs import UNIFIED_AUTISM_PROFILES, CLINICAL_SCENE_CONFIG
from autism.evaluation.enhanced_unified_evaluator import (
    run_enhanced_experiment,
    AVAILABLE_SCALES,
    DEFAULT_SCALES,
    COMPREHENSIVE_SCALES,
    get_scale_selection_recommendations
)


def page_multi_scale_assessment():
    """å¤šé‡è¡¨è¯„ä¼°é¡µé¢ - æ”¯æŒé‡è¡¨é€‰æ‹©"""
    st.header("ğŸ”¬ å¤šé‡è¡¨ç»¼åˆè¯„ä¼°")
    st.markdown("é€‰æ‹©é€‚åˆçš„è¯„ä¼°é‡è¡¨ç»„åˆï¼Œè¿›è¡Œä¸ªæ€§åŒ–çš„å­¤ç‹¬ç—‡è¯„ä¼°")
    
    # åˆ›å»ºä¸‰åˆ—å¸ƒå±€
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col1:
        st.subheader("ğŸ“‹ é‡è¡¨é€‰æ‹©")
        
        # å¿«é€Ÿé€‰æ‹©é¢„è®¾
        preset_option = st.radio(
            "é€‰æ‹©è¯„ä¼°æ–¹æ¡ˆ",
            ["è‡ªå®šä¹‰é€‰æ‹©", "æ ‡å‡†è¯„ä¼°", "å…¨é¢è¯„ä¼°", "å¿«é€Ÿç­›æŸ¥"],
            help="é€‰æ‹©é¢„è®¾æ–¹æ¡ˆæˆ–è‡ªå®šä¹‰é‡è¡¨ç»„åˆ"
        )
        
        # æ ¹æ®é¢„è®¾ç¡®å®šé‡è¡¨
        if preset_option == "æ ‡å‡†è¯„ä¼°":
            selected_scales = DEFAULT_SCALES
            st.info("âœ… ä½¿ç”¨ABCé‡è¡¨å’ŒDSM-5æ ‡å‡†")
        elif preset_option == "å…¨é¢è¯„ä¼°":
            selected_scales = COMPREHENSIVE_SCALES
            st.info("âœ… ä½¿ç”¨æ‰€æœ‰4ä¸ªé‡è¡¨")
        elif preset_option == "å¿«é€Ÿç­›æŸ¥":
            selected_scales = ['ASSQ', 'ABC']
            st.info("âœ… ä½¿ç”¨ASSQå’ŒABCå¿«é€Ÿç­›æŸ¥")
        else:
            # è‡ªå®šä¹‰é€‰æ‹©
            st.write("**é€‰æ‹©è¦ä½¿ç”¨çš„é‡è¡¨ï¼š**")
            
            scale_selections = {}
            for scale_key, scale_info in AVAILABLE_SCALES.items():
                col_check, col_info = st.columns([1, 3])
                with col_check:
                    # é»˜è®¤é€‰ä¸­ABCå’ŒDSM5
                    default_checked = scale_key in DEFAULT_SCALES
                    scale_selections[scale_key] = st.checkbox(
                        scale_info['name'],
                        value=default_checked,
                        key=f"scale_{scale_key}"
                    )
                with col_info:
                    if st.button("â„¹ï¸", key=f"info_{scale_key}"):
                        st.info(f"""
                        **{scale_info['full_name']}**
                        - ç±»å‹: {scale_info['type']}
                        - å¹´é¾„: {scale_info['age_range']}
                        - é¡¹ç›®: {scale_info['items']}
                        - æ—¶é•¿: {scale_info['time']}
                        """)
            
            selected_scales = [k for k, v in scale_selections.items() if v]
            
            if not selected_scales:
                st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè¯„ä¼°é‡è¡¨")
            else:
                st.success(f"âœ… å·²é€‰æ‹© {len(selected_scales)} ä¸ªé‡è¡¨")
        
        # æ˜¾ç¤ºé¢„è®¡æ—¶é—´
        if selected_scales:
            total_time = estimate_assessment_time(selected_scales)
            st.metric("é¢„è®¡è¯„ä¼°æ—¶é—´", f"{total_time} åˆ†é’Ÿ")
        
        # å¹´é¾„å’Œç›®çš„è¾“å…¥ï¼ˆç”¨äºæ¨èï¼‰
        with st.expander("è·å–é‡è¡¨æ¨è"):
            child_age = st.number_input("å„¿ç«¥å¹´é¾„ï¼ˆæœˆï¼‰", min_value=18, max_value=216, value=48)
            assessment_purpose = st.selectbox(
                "è¯„ä¼°ç›®çš„",
                ["diagnostic", "screening", "comprehensive"],
                format_func=lambda x: {
                    "diagnostic": "è¯Šæ–­è¯„ä¼°",
                    "screening": "ç­›æŸ¥",
                    "comprehensive": "å…¨é¢è¯„ä¼°"
                }[x]
            )
            
            if st.button("è·å–æ¨è"):
                recommendations = get_scale_selection_recommendations(
                    age=child_age,
                    purpose=assessment_purpose
                )
                st.info(f"""
                **æ¨èæ–¹æ¡ˆï¼š**
                - é‡è¡¨: {', '.join([AVAILABLE_SCALES[s]['name'] for s in recommendations['recommended_scales']])}
                - åŸå› : {recommendations['reason']}
                - é¢„è®¡æ—¶é—´: {recommendations['estimated_time']}åˆ†é’Ÿ
                """)
    
    with col2:
        st.subheader("ğŸ¯ è¯„ä¼°é…ç½®")
        
        # é€‰æ‹©ä¸¥é‡ç¨‹åº¦æ¨¡æ¿
        selected_severity = st.selectbox(
            "é€‰æ‹©ä¸¥é‡ç¨‹åº¦æ¨¡æ¿",
            list(UNIFIED_AUTISM_PROFILES.keys()),
            help="é€‰æ‹©å­¤ç‹¬ç—‡ä¸¥é‡ç¨‹åº¦é…ç½®"
        )
        
        profile = UNIFIED_AUTISM_PROFILES[selected_severity]
        
        # æ˜¾ç¤ºé…ç½®ç‰¹å¾
        with st.expander("æŸ¥çœ‹é…ç½®ç‰¹å¾", expanded=False):
            st.write(f"**é…ç½®åç§°**: {profile['name']}")
            st.write(f"**ç¤¾äº¤ç‰¹å¾**: {profile['social_characteristics']}")
            st.write(f"**æ²Ÿé€šç‰¹å¾**: {profile['communication_characteristics']}")
            st.write(f"**è¡Œä¸ºç‰¹å¾**: {profile['behavioral_characteristics']}")
        
        # é€‰æ‹©è¯„ä¼°æƒ…å¢ƒ
        selected_scene = st.selectbox("é€‰æ‹©è¯„ä¼°æƒ…å¢ƒ", list(CLINICAL_SCENE_CONFIG.keys()))
        scene_data = CLINICAL_SCENE_CONFIG[selected_scene]
        
        # æ˜¾ç¤ºåœºæ™¯ä¿¡æ¯
        with st.expander("è¯„ä¼°æƒ…å¢ƒè¯¦æƒ…"):
            st.write(f"**ç›®æ ‡**: {scene_data['target']}")
            st.write(f"**è§‚å¯Ÿè¦ç‚¹**: {', '.join(scene_data['observation_points'])}")
        
        selected_activity = st.selectbox("é€‰æ‹©è§‚å¯Ÿæ´»åŠ¨", scene_data['activities'])
        selected_trigger = st.selectbox("é€‰æ‹©è§¦å‘å› ç´ ", scene_data['triggers'])
        
        # é‡è¡¨å¯¹æ¯”é€‰é¡¹
        st.write("**è¯„ä¼°é€‰é¡¹**")
        compare_scales = st.checkbox("ç”Ÿæˆé‡è¡¨å¯¹æ¯”åˆ†æ", value=True)
        save_individual = st.checkbox("ä¿å­˜å„é‡è¡¨è¯¦ç»†ç»“æœ", value=True)
    
    with col3:
        st.subheader("ğŸš€ æ‰§è¡Œè¯„ä¼°")
        
        if not selected_scales:
            st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªè¯„ä¼°é‡è¡¨")
        else:
            # æ˜¾ç¤ºå°†è¦ä½¿ç”¨çš„é‡è¡¨
            st.write("**å°†ä½¿ç”¨ä»¥ä¸‹é‡è¡¨ï¼š**")
            for scale in selected_scales:
                st.write(f"â€¢ {AVAILABLE_SCALES[scale]['name']}")
            
            if st.button("ğŸ©º å¼€å§‹å¤šé‡è¡¨è¯„ä¼°", type="primary", use_container_width=True):
                timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]
                
                experiment_config = {
                    'template': selected_severity,
                    'scene': selected_scene,
                    'activity': selected_activity,
                    'trigger': selected_trigger,
                    'autism_profile': profile.copy(),
                    'experiment_id': f"MULTI_{selected_severity[:4]}_{timestamp}",
                    'selected_scales': selected_scales  # ä¼ é€’é€‰æ‹©çš„é‡è¡¨
                }
                
                with st.spinner(f"ğŸ¤– æ­£åœ¨è¿›è¡Œ{len(selected_scales)}ä¸ªé‡è¡¨è¯„ä¼°..."):
                    result = run_enhanced_experiment(experiment_config)
                
                if 'error' not in result:
                    st.session_state.experiment_records.append(result)
                    st.success(f"âœ… å¤šé‡è¡¨è¯„ä¼°å®Œæˆï¼ID: {result['experiment_id']}")
                    
                    # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                    display_multi_scale_results(result, compare_scales)
                    
                    # å¯¹è¯é¢„è§ˆ
                    with st.expander("æŸ¥çœ‹è¡Œä¸ºè§‚å¯Ÿå¯¹è¯", expanded=False):
                        dialogue_lines = result['dialogue'].split('\n')[:15]
                        for line in dialogue_lines:
                            if ':' in line and line.strip():
                                if 'å­¤ç‹¬ç—‡å„¿ç«¥' in line:
                                    st.markdown(f"ğŸ§’ {line}")
                                else:
                                    st.markdown(f"ğŸ‘¤ {line}")
                        
                        if len(result['dialogue'].split('\n')) > 15:
                            st.markdown("*...æŸ¥çœ‹å®Œæ•´è®°å½•è¯·å‰å¾€'è¯„ä¼°è®°å½•ç®¡ç†'*")
                else:
                    st.error(f"âŒ è¯„ä¼°å¤±è´¥: {result['error']}")


def estimate_assessment_time(selected_scales: list) -> int:
    """ä¼°ç®—è¯„ä¼°æ—¶é—´"""
    time_mapping = {
        'ABC': 15,
        'DSM5': 25,
        'CARS': 25,
        'ASSQ': 10
    }
    
    total_time = sum(time_mapping.get(scale, 15) for scale in selected_scales)
    # å¦‚æœå¤šä¸ªé‡è¡¨ï¼Œå‡å°‘ä¸€äº›æ—¶é—´ï¼ˆå› ä¸ºå…±äº«å¯¹è¯ç”Ÿæˆï¼‰
    if len(selected_scales) > 1:
        total_time = int(total_time * 0.8)
    
    return total_time


def display_multi_scale_results(result: dict, show_comparison: bool = True):
    """æ˜¾ç¤ºå¤šé‡è¡¨è¯„ä¼°ç»“æœ"""
    st.subheader("ğŸ“Š è¯„ä¼°ç»“æœ")
    
    # åˆ›å»ºæ ‡ç­¾é¡µæ˜¾ç¤ºå„é‡è¡¨ç»“æœ
    scale_tabs = []
    if 'abc_evaluation' in result:
        scale_tabs.append("ABCé‡è¡¨")
    if 'dsm5_evaluation' in result:
        scale_tabs.append("DSM-5æ ‡å‡†")
    if 'cars_evaluation' in result:
        scale_tabs.append("CARSé‡è¡¨")
    if 'assq_evaluation' in result:
        scale_tabs.append("ASSQç­›æŸ¥")
    
    if show_comparison and 'scale_comparison' in result:
        scale_tabs.append("é‡è¡¨å¯¹æ¯”")
    
    tabs = st.tabs(scale_tabs)
    tab_index = 0
    
    # ABCé‡è¡¨ç»“æœ
    if 'abc_evaluation' in result:
        with tabs[tab_index]:
            abc = result['abc_evaluation']
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»åˆ†", abc['total_score'])
            with col2:
                st.metric("ä¸¥é‡ç¨‹åº¦", abc['severity'])
            with col3:
                st.metric("ä¸´åºŠæ„ä¹‰", "é˜³æ€§" if abc['clinical_range'] else "é˜´æ€§")
            
            # é¢†åŸŸå¾—åˆ†
            st.write("**é¢†åŸŸå¾—åˆ†ï¼š**")
            domain_df = pd.DataFrame.from_dict(abc['domain_scores'], orient='index', columns=['å¾—åˆ†'])
            st.dataframe(domain_df, use_container_width=True)
            
            # è¯†åˆ«çš„è¡Œä¸º
            if abc['identified_behaviors']:
                st.write("**è¯†åˆ«çš„ä¸»è¦è¡Œä¸ºï¼š**")
                for domain, behaviors in abc['identified_behaviors'].items():
                    if behaviors:
                        st.write(f"â€¢ {domain}: {', '.join(behaviors[:3])}")
        tab_index += 1
    
    # DSM-5æ ‡å‡†ç»“æœ
    if 'dsm5_evaluation' in result:
        with tabs[tab_index]:
            dsm5 = result['dsm5_evaluation']
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ ¸å¿ƒç—‡çŠ¶", f"{dsm5['core_symptom_average']:.2f}")
            with col2:
                st.metric("ä¸¥é‡ç¨‹åº¦", dsm5['severity_level'])
            with col3:
                criteria_met = dsm5['meets_criteria']['overall']
                st.metric("ç¬¦åˆæ ‡å‡†", "æ˜¯" if criteria_met else "å¦")
            
            # å„ç»´åº¦å¾—åˆ†
            st.write("**ç—‡çŠ¶ç»´åº¦è¯„åˆ†ï¼š**")
            scores_df = pd.DataFrame.from_dict(dsm5['scores'], orient='index', columns=['è¯„åˆ†'])
            st.dataframe(scores_df, use_container_width=True)
            
            # ä¸´åºŠè§‚å¯Ÿ
            if dsm5['clinical_observations']:
                st.write("**ä¸´åºŠè§‚å¯Ÿè¦ç‚¹ï¼š**")
                for category, observations in dsm5['clinical_observations'].items():
                    if observations:
                        st.write(f"â€¢ {category}: {', '.join(observations[:3])}")
        tab_index += 1
    
    # CARSé‡è¡¨ç»“æœ
    if 'cars_evaluation' in result:
        with tabs[tab_index]:
            cars = result['cars_evaluation']
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»åˆ†", f"{cars['total_score']:.1f}")
            with col2:
                st.metric("ä¸¥é‡ç¨‹åº¦", cars['severity'])
            with col3:
                st.metric("ä¸´åºŠæ„ä¹‰", "é˜³æ€§" if cars['clinical_cutoff'] else "é˜´æ€§")
            
            # é¡¹ç›®å¾—åˆ†çƒ­åŠ›å›¾
            st.write("**CARSå„é¡¹ç›®è¯„åˆ†ï¼š**")
            items_df = pd.DataFrame.from_dict(cars['item_scores'], orient='index', columns=['è¯„åˆ†'])
            items_df = items_df.sort_values('è¯„åˆ†', ascending=False)
            st.dataframe(
                items_df.style.background_gradient(cmap='RdYlGn_r', vmin=1, vmax=4),
                use_container_width=True
            )
            
            # è§£é‡Šå’Œå»ºè®®
            if 'interpretation' in cars:
                interp = cars['interpretation']
                st.info(f"**ä¸´åºŠæ„ä¹‰**: {interp['clinical_significance']}")
                if interp['recommendations']:
                    st.write("**å»ºè®®ï¼š**")
                    for rec in interp['recommendations']:
                        st.write(f"â€¢ {rec}")
        tab_index += 1
    
    # ASSQç­›æŸ¥ç»“æœ
    if 'assq_evaluation' in result:
        with tabs[tab_index]:
            assq = result['assq_evaluation']
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»åˆ†", assq['total_score'])
            with col2:
                st.metric("ç­›æŸ¥ç»“æœ", assq['screening_result']['screening_result'])
            with col3:
                st.metric("é£é™©ç­‰çº§", assq['risk_level'])
            
            # ç±»åˆ«å¾—åˆ†
            st.write("**å„ç±»åˆ«å¾—åˆ†ï¼š**")
            category_df = pd.DataFrame.from_dict(assq['category_scores'], orient='index', columns=['å¾—åˆ†'])
            st.dataframe(category_df, use_container_width=True)
            
            # ç±»åˆ«è§£é‡Š
            if 'category_interpretation' in assq:
                st.write("**ç±»åˆ«è¯„ä¼°ï¼š**")
                for category, interpretation in assq['category_interpretation'].items():
                    st.write(f"â€¢ {category}: {interpretation}")
            
            # ç­›æŸ¥å»ºè®®
            screening = assq['screening_result']
            if screening['recommendations']:
                st.write("**ç­›æŸ¥å»ºè®®ï¼š**")
                for rec in screening['recommendations']:
                    st.write(f"â€¢ {rec}")
        tab_index += 1
    
    # é‡è¡¨å¯¹æ¯”
    if show_comparison and 'scale_comparison' in result:
        with tabs[tab_index]:
            comparison = result['scale_comparison']
            
            # ä¸€è‡´æ€§åˆ†æ
            if 'consistency' in comparison and comparison['consistency']:
                st.write("**é‡è¡¨ä¸€è‡´æ€§ï¼š**")
                st.info(f"æ•´ä½“ä¸€è‡´æ€§: {comparison['consistency'].get('overall', 'æœªè¯„ä¼°')}")
            
            # ä¸¥é‡ç¨‹åº¦å¯¹æ¯”
            if 'severity_agreement' in comparison and comparison['severity_agreement']:
                st.write("**å„é‡è¡¨ä¸¥é‡ç¨‹åº¦åˆ¤æ–­ï¼š**")
                severity_df = pd.DataFrame.from_dict(
                    comparison['severity_agreement'], 
                    orient='index', 
                    columns=['ä¸¥é‡ç¨‹åº¦']
                )
                st.dataframe(severity_df, use_container_width=True)
            
            # å…³é”®å‘ç°
            if 'key_findings' in comparison and comparison['key_findings']:
                st.write("**ç»¼åˆå‘ç°ï¼š**")
                for finding in comparison['key_findings']:
                    st.write(f"â€¢ {finding}")
            
            # è¯„ä¼°æ‘˜è¦
            if 'evaluation_summary' in result:
                summary = result['evaluation_summary']
                st.write("**è¯„ä¼°æ‘˜è¦ï¼š**")
                if summary['severity_consensus']:
                    st.info(f"**å…±è¯†ç»“æœ**: {summary['severity_consensus']}")
                if summary['recommendations']:
                    st.write("**ç»¼åˆå»ºè®®ï¼š**")
                    for rec in summary['recommendations']:
                        st.write(f"â€¢ {rec}")