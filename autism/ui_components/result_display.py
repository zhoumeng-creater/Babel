"""ç»“æœæ˜¾ç¤ºç»„ä»¶"""
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px

from autism.config import ABC_EVALUATION_METRICS, DSM5_EVALUATION_METRICS


def display_dual_assessment_results(result):
    """æ˜¾ç¤ºåŒé‡è¯„ä¼°ç»“æœ"""
    st.subheader("ğŸ“Š åŒé‡è¯„ä¼°ç»“æœå¯¹æ¯”")
    
    # åˆ›å»ºä¸¤åˆ—æ˜¾ç¤ºä¸¤ç§è¯„ä¼°ç»“æœ
    col_abc, col_dsm5 = st.columns(2)
    
    with col_abc:
        st.markdown("### ğŸ“‹ ABCé‡è¡¨è¯„ä¼°")
        
        # ABCè¯„ä¼°ç»“æœ
        abc_eval = result['abc_evaluation']
        total_score = abc_eval['total_score']
        severity = abc_eval['severity']
        
        if total_score >= 67:
            st.error(f"**ABCæ€»åˆ†: {total_score}**")
            st.error(f"**åˆ¤å®š: {severity}**")
        elif total_score >= 53:
            st.warning(f"**ABCæ€»åˆ†: {total_score}**")
            st.warning(f"**åˆ¤å®š: {severity}**")
        else:
            st.info(f"**ABCæ€»åˆ†: {total_score}**")
            st.info(f"**åˆ¤å®š: {severity}**")
        
        st.write("**å„é¢†åŸŸå¾—åˆ†**:")
        for domain, score in abc_eval['domain_scores'].items():
            max_score = ABC_EVALUATION_METRICS[domain]['max_score']
            percentage = score / max_score * 100
            st.write(f"â€¢ {domain}: {score}/{max_score} ({percentage:.0f}%)")
        
        # æ˜¾ç¤ºè¯†åˆ«åˆ°çš„ä¸»è¦è¡Œä¸º
        if abc_eval['identified_behaviors']:
            st.write("**è¯†åˆ«åˆ°çš„ä¸»è¦è¡Œä¸º**:")
            behavior_count = 0
            for domain, behaviors in abc_eval['identified_behaviors'].items():
                if behaviors and behavior_count < 5:
                    for behavior in behaviors[:2]:
                        st.write(f"â€¢ {behavior}")
                        behavior_count += 1
                        if behavior_count >= 5:
                            break
    
    with col_dsm5:
        st.markdown("### ğŸ§  DSM-5æ ‡å‡†è¯„ä¼°")
        
        # DSM-5è¯„ä¼°ç»“æœ
        dsm5_eval = result['dsm5_evaluation']
        core_avg = dsm5_eval['core_symptom_average']
        
        if core_avg >= 4.0:
            st.error(f"**æ ¸å¿ƒç—‡çŠ¶å‡å€¼: {core_avg:.2f}/5**")
            st.error("**éœ€è¦éå¸¸å¤§é‡æ”¯æŒ**")
        elif core_avg >= 3.0:
            st.warning(f"**æ ¸å¿ƒç—‡çŠ¶å‡å€¼: {core_avg:.2f}/5**")
            st.warning("**éœ€è¦å¤§é‡æ”¯æŒ**")
        else:
            st.info(f"**æ ¸å¿ƒç—‡çŠ¶å‡å€¼: {core_avg:.2f}/5**")
            st.info("**éœ€è¦æ”¯æŒ**")
        
        st.write("**å„ç»´åº¦è¯„åˆ†**:")
        for metric, score in dsm5_eval['scores'].items():
            severity_label = "è½»åº¦" if score < 2.5 else "ä¸­åº¦" if score < 3.5 else "é‡åº¦"
            st.write(f"â€¢ {metric}: {score:.2f}/5 ({severity_label})")
        
        # æ˜¾ç¤ºä¸´åºŠè§‚å¯Ÿ
        if dsm5_eval['clinical_observations']:
            st.write("**ä¸´åºŠè§‚å¯Ÿè¦ç‚¹**:")
            obs_count = 0
            for category, observations in dsm5_eval['clinical_observations'].items():
                if observations and obs_count < 5:
                    for obs in observations[:1]:
                        st.write(f"â€¢ [{category}] {obs}")
                        obs_count += 1
                        if obs_count >= 5:
                            break
    
    # è¯„ä¼°ä¸€è‡´æ€§åˆ†æ
    st.subheader("ğŸ”„ è¯„ä¼°ä¸€è‡´æ€§åˆ†æ")
    
    # ç®€å•çš„ä¸€è‡´æ€§åˆ¤æ–­
    abc_severe = total_score >= 67
    dsm5_severe = core_avg >= 3.5
    
    if abc_severe == dsm5_severe:
        st.success("âœ… ä¸¤ç§è¯„ä¼°æ ‡å‡†çš„ä¸¥é‡ç¨‹åº¦åˆ¤æ–­åŸºæœ¬ä¸€è‡´")
    else:
        st.warning("âš ï¸ ä¸¤ç§è¯„ä¼°æ ‡å‡†çš„ä¸¥é‡ç¨‹åº¦åˆ¤æ–­å­˜åœ¨å·®å¼‚")
        if abc_severe and not dsm5_severe:
            st.info("ABCé‡è¡¨æ˜¾ç¤ºè¾ƒä¸¥é‡ï¼Œä½†DSM-5è¯„ä¼°ç›¸å¯¹è¾ƒè½»")
        else:
            st.info("DSM-5è¯„ä¼°æ˜¾ç¤ºè¾ƒä¸¥é‡ï¼Œä½†ABCé‡è¡¨å¾—åˆ†ç›¸å¯¹è¾ƒä½")


def analyze_batch_consistency(results):
    """åˆ†ææ‰¹é‡ç»“æœçš„è¯„ä¼°ä¸€è‡´æ€§"""
    consistent_count = 0
    abc_scores = []
    dsm5_scores = []
    
    for record in results:
        abc_severe = record['abc_evaluation']['total_score'] >= 67
        dsm5_severe = record['dsm5_evaluation']['core_symptom_average'] >= 3.5
        
        if abc_severe == dsm5_severe:
            consistent_count += 1
        
        # æ ‡å‡†åŒ–åˆ†æ•°ä»¥ä¾¿è®¡ç®—ç›¸å…³æ€§
        abc_normalized = record['abc_evaluation']['total_score'] / 158  # ABCæœ€é«˜åˆ†158
        dsm5_normalized = record['dsm5_evaluation']['core_symptom_average'] / 5  # DSM-5æœ€é«˜5åˆ†
        
        abc_scores.append(abc_normalized)
        dsm5_scores.append(dsm5_normalized)
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    if len(results) > 1:
        correlation = np.corrcoef(abc_scores, dsm5_scores)[0, 1]
    else:
        correlation = 0
    
    return {
        'consistency_rate': (consistent_count / len(results)) * 100 if results else 0,
        'correlation': correlation,
        'abc_scores': abc_scores,
        'dsm5_scores': dsm5_scores
    }


def create_severity_comparison_df(results):
    """åˆ›å»ºä¸¥é‡ç¨‹åº¦å¯¹æ¯”æ•°æ®æ¡†"""
    comparison_data = []
    
    # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
    severity_groups = {}
    for record in results:
        severity = record.get('template', 'è‡ªå®šä¹‰')
        if severity not in severity_groups:
            severity_groups[severity] = {
                'abc_scores': [],
                'dsm5_scores': []
            }
        
        severity_groups[severity]['abc_scores'].append(record['abc_evaluation']['total_score'])
        severity_groups[severity]['dsm5_scores'].append(record['dsm5_evaluation']['core_symptom_average'])
    
    # è®¡ç®—æ¯ç»„çš„ç»Ÿè®¡æ•°æ®
    for severity, data in severity_groups.items():
        comparison_data.append({
            'ä¸¥é‡ç¨‹åº¦': severity,
            'æ ·æœ¬æ•°': len(data['abc_scores']),
            'ABCå¹³å‡åˆ†': f"{np.mean(data['abc_scores']):.1f}",
            'ABCæ ‡å‡†å·®': f"{np.std(data['abc_scores']):.1f}",
            'DSM5å¹³å‡åˆ†': f"{np.mean(data['dsm5_scores']):.2f}",
            'DSM5æ ‡å‡†å·®': f"{np.std(data['dsm5_scores']):.2f}"
        })
    
    return pd.DataFrame(comparison_data)


def display_abc_detailed_results(abc_eval):
    """æ˜¾ç¤ºABCè¯„ä¼°è¯¦ç»†ç»“æœ"""
    st.write(f"### ABCæ€»åˆ†: {abc_eval['total_score']}")
    st.write(f"### ä¸¥é‡ç¨‹åº¦: {abc_eval['severity']}")
    
    # å„é¢†åŸŸå¾—åˆ†é›·è¾¾å›¾
    domain_scores = abc_eval['domain_scores']
    domain_names = list(domain_scores.keys())
    domain_values = list(domain_scores.values())
    domain_max_values = [ABC_EVALUATION_METRICS[d]['max_score'] for d in domain_names]
    domain_percentages = [v/m*100 for v, m in zip(domain_values, domain_max_values)]
    
    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(
        r=domain_percentages,
        theta=[d.replace("å¾—åˆ†", "") for d in domain_names],
        fill='toself',
        name='å¾—åˆ†ç™¾åˆ†æ¯”'
    ))
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100]
            )),
        showlegend=False,
        title="ABCå„é¢†åŸŸå¾—åˆ†ç™¾åˆ†æ¯”"
    )
    st.plotly_chart(fig, use_container_width=True)
    
    # è¯†åˆ«åˆ°çš„è¡Œä¸º
    if abc_eval['identified_behaviors']:
        st.write("### è¯†åˆ«åˆ°çš„è¡Œä¸º")
        for domain, behaviors in abc_eval['identified_behaviors'].items():
            if behaviors:
                st.write(f"**{domain}**:")
                for behavior in behaviors:
                    st.write(f"â€¢ {behavior}")


def display_dsm5_detailed_results(dsm5_eval):
    """æ˜¾ç¤ºDSM-5è¯„ä¼°è¯¦ç»†ç»“æœ"""
    st.write(f"### æ ¸å¿ƒç—‡çŠ¶å¹³å‡åˆ†: {dsm5_eval['core_symptom_average']:.2f}/5")
    
    # å„ç»´åº¦è¯„åˆ†æ¡å½¢å›¾
    scores = dsm5_eval['scores']
    
    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=list(scores.keys()),
        y=list(scores.values()),
        marker_color=['red' if v >= 4 else 'orange' if v >= 3 else 'green' for v in scores.values()]
    ))
    fig.update_layout(
        title="DSM-5å„ç»´åº¦è¯„åˆ†",
        yaxis_range=[0, 5],
        yaxis_title="ä¸¥é‡ç¨‹åº¦ (1-5åˆ†)"
    )
    st.plotly_chart(fig, use_container_width=True)
    
    # ä¸´åºŠè§‚å¯Ÿ
    if dsm5_eval['clinical_observations']:
        st.write("### ä¸´åºŠè§‚å¯Ÿè¦ç‚¹")
        for category, observations in dsm5_eval['clinical_observations'].items():
            if observations:
                st.write(f"**{category}**:")
                for obs in observations:
                    st.write(f"â€¢ {obs}")


def display_assessment_comparison(record):
    """æ˜¾ç¤ºå•ä¸ªè®°å½•çš„è¯„ä¼°å¯¹æ¯”"""
    abc_eval = record['abc_evaluation']
    dsm5_eval = record['dsm5_evaluation']
    
    # ä¸¥é‡ç¨‹åº¦å¯¹æ¯”
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("ABCæ€»åˆ†", abc_eval['total_score'])
        st.write(f"åˆ¤å®š: {abc_eval['severity']}")
    
    with col2:
        st.metric("DSM-5æ ¸å¿ƒç—‡çŠ¶", f"{dsm5_eval['core_symptom_average']:.2f}")
        severity_label = "é‡åº¦" if dsm5_eval['core_symptom_average'] >= 3.5 else "ä¸­åº¦" if dsm5_eval['core_symptom_average'] >= 2.5 else "è½»åº¦"
        st.write(f"åˆ¤å®š: {severity_label}")
    
    # ä¸€è‡´æ€§åˆ¤æ–­
    abc_severe = abc_eval['total_score'] >= 67
    dsm5_severe = dsm5_eval['core_symptom_average'] >= 3.5
    
    if abc_severe == dsm5_severe:
        st.success("âœ… ä¸¤ç§è¯„ä¼°æ ‡å‡†çš„ä¸¥é‡ç¨‹åº¦åˆ¤æ–­ä¸€è‡´")
    else:
        st.warning("âš ï¸ ä¸¤ç§è¯„ä¼°æ ‡å‡†çš„ä¸¥é‡ç¨‹åº¦åˆ¤æ–­å­˜åœ¨å·®å¼‚")
    
    # è¯¦ç»†å¯¹æ¯”
    st.write("### è¯„ä¼°ç‰¹ç‚¹å¯¹æ¯”")
    
    comparison_text = []
    
    # ABCç‰¹ç‚¹
    if 'identified_behaviors' in abc_eval:
        total_behaviors = sum(len(behaviors) for behaviors in abc_eval['identified_behaviors'].values())
        comparison_text.append(f"â€¢ ABCè¯†åˆ«äº† {total_behaviors} ä¸ªå…·ä½“è¡Œä¸º")
    
    # DSM-5ç‰¹ç‚¹
    if 'clinical_observations' in dsm5_eval:
        total_observations = sum(len(obs) for obs in dsm5_eval['clinical_observations'].values())
        comparison_text.append(f"â€¢ DSM-5è®°å½•äº† {total_observations} ç±»ä¸´åºŠè§‚å¯Ÿ")
    
    # ä¸»è¦å·®å¼‚
    if abc_eval['domain_scores'].get('è¯­è¨€é¢†åŸŸå¾—åˆ†', 0) > 30 and dsm5_eval['scores'].get('æ²Ÿé€šäº¤æµèƒ½åŠ›', 0) < 3:
        comparison_text.append("â€¢ ABCæ˜¾ç¤ºè¯­è¨€é—®é¢˜ä¸¥é‡ï¼Œä½†DSM-5è¯„ä¼°ç›¸å¯¹è¾ƒè½»")
    
    if abc_eval['domain_scores'].get('äº¤å¾€é¢†åŸŸå¾—åˆ†', 0) > 30 and dsm5_eval['scores'].get('ç¤¾äº¤äº’åŠ¨è´¨é‡', 0) < 3:
        comparison_text.append("â€¢ ABCæ˜¾ç¤ºç¤¾äº¤éšœç¢ä¸¥é‡ï¼Œä½†DSM-5è¯„ä¼°ç›¸å¯¹è¾ƒè½»")
    
    for text in comparison_text:
        st.write(text)


def display_single_record_analysis(record):
    """æ˜¾ç¤ºå•ä¸ªè®°å½•çš„è¯¦ç»†åˆ†æ"""
    st.write("### ğŸ“Š è¯¦ç»†è¯„ä¼°åˆ†æ")
    
    # åˆ›å»ºä¸¤åˆ—
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ABCé‡è¡¨åˆ†æ**")
        # é¥¼å›¾æ˜¾ç¤ºå„é¢†åŸŸè´¡çŒ®
        domain_scores = record['abc_evaluation']['domain_scores']
        fig = px.pie(
            values=list(domain_scores.values()),
            names=[d.replace("å¾—åˆ†", "") for d in domain_scores.keys()],
            title="ABCå„é¢†åŸŸå¾—åˆ†å æ¯”"
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.write("**DSM-5æ ‡å‡†åˆ†æ**")
        # é›·è¾¾å›¾æ˜¾ç¤ºå„ç»´åº¦
        scores = record['dsm5_evaluation']['scores']
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(
            r=list(scores.values()),
            theta=list(scores.keys()),
            fill='toself'
        ))
        fig.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[1, 5])),
            title="DSM-5å„ç»´åº¦è¯„åˆ†"
        )
        st.plotly_chart(fig, use_container_width=True)